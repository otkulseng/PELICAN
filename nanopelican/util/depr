#!/usr/bin/env python

import argparse
import yaml
import logging
from pathlib import Path
import os
import pickle
import numpy as np

from nanopelican.models import PelicanNano, Pelican
from nanopelican.data import load_dataset
from nanopelican.schedulers import LinearWarmupCosineAnnealing

import tensorflow as tf
from tqdm.keras import TqdmCallback


import tensorflow as tf

logger = logging.getLogger('')



def save_experiment(folder, model, history):
    model.save(folder / 'model.keras')

    with open(folder / 'history.pkl', 'wb') as file_pi:
        mydict = history
        pickle.dump(mydict, file_pi)




def run(conf):

    save_dir = create_directory(conf['save_dir'])
    # Save config to directory

    conf = add_default_values(conf)





    # Initialize logger
    logger.setLevel(logging.DEBUG)
    logfile = logging.FileHandler(save_dir / 'log.log')
    logging.basicConfig(level=logging.DEBUG, handlers=[logfile])
    train_log = tf.keras.callbacks.CSVLogger(save_dir / 'training.log')


    model = Pelican(conf['model'])

    dataset = load_dataset(conf['dataset'], keys=['train', 'valid'])

    hyperparams = conf['hyperparams']

    if True:
        data = dataset.train
        features, _ = data[0]
        model.build(features.shape)
        model.summary(features.shape)

    if model.output_shape[-1] > 1:
        loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
    else:
        loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=LinearWarmupCosineAnnealing(
            epochs=hyperparams['epochs'],
            steps_per_epoch=len(dataset.train),
        )),
        loss=loss,
        metrics=['accuracy'],
    )

    dataset.val.shuffle(keepratio=True)
    try:
        history = model.fit(
            dataset.train.shuffle(keepratio=True).batch(hyperparams['batch_size']),
            epochs = hyperparams['epochs'],
            validation_data = (dataset.val.x_data[:hyperparams['val_size']], dataset.val.y_data[:hyperparams['val_size']]),
            callbacks=[TqdmCallback(verbose=hyperparams['verbose']), train_log, tf.keras.callbacks.ModelCheckpoint(
                filepath = save_dir / 'model.best_acc.keras',
                monitor='val_accuracy',
                mode='max',
                save_best_only=True
            ), tf.keras.callbacks.ModelCheckpoint(
                filepath = save_dir / 'model.best_loss.keras',
                monitor='val_loss',
                mode='min',
                save_best_only=True
            ), tf.keras.callbacks.EarlyStopping(
                monitor="val_accuracy",
                patience=20,
                mode="max",
            )
        ],
            verbose=0
        )
        history = history.history

    except KeyboardInterrupt:
        logger.info("Keyboard Interrupt! Saving progress")
        history = {}

    save_experiment(save_dir, model, history)


def main():
    conf = load_arguments()
    return run(conf)

if __name__ == '__main__':
    main()

