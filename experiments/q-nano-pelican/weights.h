#include "../nPELICAN.h"


// WARNING: Have been trained with the following assumptions: 
// input_t == ap_fixed<20,12> 
// internal_t == ap_fixed<20,12> 
// weight_t=bias_t == ap_fixed<11,3> 

//normalization constants
//these are currently NOT quantized, so should be FP
internal_t const2v0 = 0.19561107427281468;
internal_t const2v02 = 0.03826369237816462;

internal_t const2v2 = 0.10615466134511138;
internal_t const2v22 = 0.011268812125295284;

//first batchnorm [mean, weight/sqrt(var), bias]
weight_t batch1_2to2[3] = { 3.996093750000000,  0.035156250000000, -0.925781250000000};

//2to2 linear layer
weight_t w1_2to2[NHIDDEN*6] = { 1.222656250000000, -0.011718750000000, -0.554687500000000, -0.863281250000000,  0.402343750000000, -0.621093750000000,  0.699218750000000,  0.195312500000000, -0.546875000000000,  0.332031250000000, -0.726562500000000,  0.312500000000000};
bias_t b1_2to2[NHIDDEN] = {0.113281250000000, 0.328125000000000};
bias_t b1_diag_2to2[NHIDDEN] = {-0.945312500000000,  1.156250000000000};

//second batchnorm [channel][mean, weight/sqrt(var), bias]
weight_t batch2_2to0[NHIDDEN][3] = {{ 0.464843750000000,  2.260709285736084,  0.496093750000000}, { 0.687500000000000,  1.990244865417480, -0.425781250000000}};

//2to1 linear layer
weight_t w2_2to0[NHIDDEN*2*NOUT] = {-0.367187500000000,  0.875000000000000,  2.035156250000000,  1.460937500000000, -1.000000000000000,  0.316406250000000,  0.886718750000000,  0.789062500000000,  0.644531250000000, -0.667968750000000,  1.429687500000000,  0.714843750000000, -1.011718750000000, -0.167968750000000,  1.683593750000000,  0.574218750000000,  0.246093750000000, -1.214843750000000, -0.800781250000000,  0.449218750000000};
bias_t b2_2to0[NOUT] = {-0.152343750000000, -0.300781250000000,  1.113281250000000,  0.750000000000000, -0.964843750000000};
