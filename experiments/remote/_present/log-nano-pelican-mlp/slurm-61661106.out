2024-06-08 17:34:09.974723: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-06-08 17:34:10.028330: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2024-06-08 17:34:10.269026: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-08 17:34:12.680576: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-06-08 17:34:21.256294: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected


Running train for /cluster/home/okulseng/PELICAN/experiments/_present/log-nano-pelican-mlp/conf-pelican-3/config.yml


Model: "functional_1"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input_layer         │ (None, 32, 4)     │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ inner_product       │ [(None, 32, 32,   │          0 │ input_layer[0][0] │
│ (InnerProduct)      │ 1), (None, 32,    │            │                   │
│                     │ 32, 1)]           │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ log_layer           │ (None, 32, 32, 1) │          1 │ inner_product[0]… │
│ (LogLayer)          │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalization │ (None, 32, 32, 1) │          4 │ log_layer[0][0]   │
│ (BatchNormalizatio… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ lineq2v2 (Lineq2v2) │ (None, 32, 32, 6) │          0 │ batch_normalizat… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ diag_bias_dense     │ (None, 32, 32, 8) │         64 │ lineq2v2[0][0]    │
│ (DiagBiasDense)     │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ multiply (Multiply) │ (None, 32, 32, 8) │          0 │ diag_bias_dense[… │
│                     │                   │            │ inner_product[0]… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 32, 32, 8) │         32 │ multiply[0][0]    │
│ (BatchNormalizatio… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ lineq2v0 (Lineq2v0) │ (None, 16)        │          0 │ batch_normalizat… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense (Dense)       │ (None, 5)         │         85 │ lineq2v0[0][0]    │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 186 (744.00 B)
 Trainable params: 168 (672.00 B)
 Non-trainable params: 18 (72.00 B)
'LogLayer' object has no attribute 'calc_flops'
0epoch [00:00, ?epoch/s]  0%|          | 0/1000 [00:00<?, ?epoch/s]/cluster/home/okulseng/.local/lib64/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.
  self._warn_if_super_not_called()
  0%|          | 1/1000 [07:30<125:06:53, 450.86s/epoch, accuracy=0.578, loss=1.13, lr=0.002, val_accuracy=0.664, val_loss=0.982, val_lr=0.002]  0%|          | 2/1000 [14:44<122:13:50, 440.91s/epoch, accuracy=0.655, loss=0.97, lr=0.002, val_accuracy=0.662, val_loss=0.956, val_lr=0.002]  0%|          | 3/1000 [21:55<120:52:15, 436.44s/epoch, accuracy=0.657, loss=0.955, lr=0.002, val_accuracy=0.664, val_loss=0.943, val_lr=0.002]  0%|          | 4/1000 [29:18<121:23:27, 438.76s/epoch, accuracy=0.658, loss=0.944, lr=0.002, val_accuracy=0.661, val_loss=0.938, val_lr=0.002]  0%|          | 5/1000 [36:38<121:23:48, 439.22s/epoch, accuracy=0.661, loss=0.933, lr=0.002, val_accuracy=0.667, val_loss=0.927, val_lr=0.002]  1%|          | 6/1000 [44:05<122:00:42, 441.89s/epoch, accuracy=0.664, loss=0.927, lr=0.002, val_accuracy=0.673, val_loss=0.921, val_lr=0.002]  1%|          | 7/1000 [51:31<122:13:53, 443.14s/epoch, accuracy=0.665, loss=0.922, lr=0.002, val_accuracy=0.672, val_loss=0.911, val_lr=0.002]  1%|          | 8/1000 [58:54<122:07:27, 443.19s/epoch, accuracy=0.667, loss=0.918, lr=0.002, val_accuracy=0.67, val_loss=0.915, val_lr=0.002]   1%|          | 9/1000 [1:06:08<121:11:41, 440.26s/epoch, accuracy=0.668, loss=0.915, lr=0.002, val_accuracy=0.677, val_loss=0.907, val_lr=0.002]  1%|          | 10/1000 [1:12:49<117:45:39, 428.22s/epoch, accuracy=0.669, loss=0.911, lr=0.002, val_accuracy=0.676, val_loss=0.898, val_lr=0.002]  1%|          | 11/1000 [1:19:32<115:29:19, 420.38s/epoch, accuracy=0.671, loss=0.908, lr=0.002, val_accuracy=0.681, val_loss=0.899, val_lr=0.002]  1%|          | 12/1000 [1:26:15<113:59:39, 415.36s/epoch, accuracy=0.671, loss=0.906, lr=0.002, val_accuracy=0.668, val_loss=0.915, val_lr=0.002]  1%|▏         | 13/1000 [1:33:02<113:06:37, 412.56s/epoch, accuracy=0.672, loss=0.904, lr=0.002, val_accuracy=0.683, val_loss=0.893, val_lr=0.002]  1%|▏         | 14/1000 [1:39:45<112:16:29, 409.93s/epoch, accuracy=0.675, loss=0.902, lr=0.002, val_accuracy=0.675, val_loss=0.896, val_lr=0.002]  2%|▏         | 15/1000 [1:46:31<111:47:26, 408.58s/epoch, accuracy=0.676, loss=0.899, lr=0.002, val_accuracy=0.678, val_loss=0.893, val_lr=0.002]  2%|▏         | 16/1000 [1:53:14<111:12:58, 406.89s/epoch, accuracy=0.677, loss=0.895, lr=0.002, val_accuracy=0.684, val_loss=0.887, val_lr=0.002]  2%|▏         | 17/1000 [1:59:58<110:54:39, 406.18s/epoch, accuracy=0.677, loss=0.894, lr=0.002, val_accuracy=0.687, val_loss=0.88, val_lr=0.002]   2%|▏         | 18/1000 [2:06:41<110:32:07, 405.22s/epoch, accuracy=0.678, loss=0.892, lr=0.002, val_accuracy=0.679, val_loss=0.891, val_lr=0.002]  2%|▏         | 19/1000 [2:13:26<110:23:03, 405.08s/epoch, accuracy=0.678, loss=0.89, lr=0.002, val_accuracy=0.689, val_loss=0.877, val_lr=0.002]   2%|▏         | 20/1000 [2:20:11<110:15:59, 405.06s/epoch, accuracy=0.679, loss=0.889, lr=0.002, val_accuracy=0.67, val_loss=0.892, val_lr=0.002]  2%|▏         | 21/1000 [2:26:59<110:21:17, 405.80s/epoch, accuracy=0.679, loss=0.889, lr=0.002, val_accuracy=0.689, val_loss=0.877, val_lr=0.002]  2%|▏         | 22/1000 [2:33:44<110:13:43, 405.75s/epoch, accuracy=0.679, loss=0.888, lr=0.002, val_accuracy=0.689, val_loss=0.875, val_lr=0.002]  2%|▏         | 23/1000 [2:40:24<109:39:15, 404.05s/epoch, accuracy=0.68, loss=0.888, lr=0.002, val_accuracy=0.691, val_loss=0.872, val_lr=0.002]   2%|▏         | 24/1000 [2:46:55<108:27:11, 400.03s/epoch, accuracy=0.679, loss=0.887, lr=0.002, val_accuracy=0.685, val_loss=0.879, val_lr=0.002]  2%|▎         | 25/1000 [2:53:24<107:25:30, 396.65s/epoch, accuracy=0.68, loss=0.886, lr=0.002, val_accuracy=0.684, val_loss=0.88, val_lr=0.002]    3%|▎         | 26/1000 [2:59:46<106:09:01, 392.34s/epoch, accuracy=0.68, loss=0.886, lr=0.002, val_accuracy=0.685, val_loss=0.881, val_lr=0.002]  3%|▎         | 27/1000 [3:05:53<104:00:25, 384.82s/epoch, accuracy=0.681, loss=0.885, lr=0.002, val_accuracy=0.68, val_loss=0.883, val_lr=0.002]  3%|▎         | 28/1000 [3:12:04<102:43:46, 380.48s/epoch, accuracy=0.681, loss=0.885, lr=0.002, val_accuracy=0.688, val_loss=0.88, val_lr=0.002]  3%|▎         | 29/1000 [3:18:13<101:43:09, 377.13s/epoch, accuracy=0.681, loss=0.885, lr=0.002, val_accuracy=0.683, val_loss=0.877, val_lr=0.002]  3%|▎         | 30/1000 [3:24:17<100:33:51, 373.23s/epoch, accuracy=0.681, loss=0.884, lr=0.002, val_accuracy=0.689, val_loss=0.872, val_lr=0.002]  3%|▎         | 31/1000 [3:30:20<99:35:35, 370.01s/epoch, accuracy=0.682, loss=0.883, lr=0.002, val_accuracy=0.689, val_loss=0.872, val_lr=0.002]   3%|▎         | 32/1000 [3:36:23<98:56:32, 367.97s/epoch, accuracy=0.682, loss=0.883, lr=0.002, val_accuracy=0.69, val_loss=0.869, val_lr=0.002]   3%|▎         | 33/1000 [3:42:26<98:28:14, 366.59s/epoch, accuracy=0.682, loss=0.883, lr=0.002, val_accuracy=0.692, val_loss=0.87, val_lr=0.002]  3%|▎         | 34/1000 [3:48:30<98:06:41, 365.63s/epoch, accuracy=0.683, loss=0.882, lr=0.002, val_accuracy=0.692, val_loss=0.868, val_lr=0.002]  4%|▎         | 35/1000 [3:54:34<97:54:11, 365.24s/epoch, accuracy=0.683, loss=0.882, lr=0.002, val_accuracy=0.692, val_loss=0.87, val_lr=0.002]   4%|▎         | 36/1000 [4:00:36<97:31:15, 364.19s/epoch, accuracy=0.683, loss=0.882, lr=0.002, val_accuracy=0.693, val_loss=0.87, val_lr=0.002]  4%|▎         | 37/1000 [4:06:38<97:16:17, 363.63s/epoch, accuracy=0.683, loss=0.882, lr=0.002, val_accuracy=0.684, val_loss=0.876, val_lr=0.002]  4%|▍         | 38/1000 [4:12:40<97:02:22, 363.14s/epoch, accuracy=0.683, loss=0.882, lr=0.002, val_accuracy=0.692, val_loss=0.867, val_lr=0.002]  4%|▍         | 39/1000 [4:18:42<96:50:21, 362.77s/epoch, accuracy=0.684, loss=0.881, lr=0.002, val_accuracy=0.691, val_loss=0.872, val_lr=0.002]  4%|▍         | 40/1000 [4:24:44<96:41:59, 362.62s/epoch, accuracy=0.684, loss=0.88, lr=0.002, val_accuracy=0.693, val_loss=0.868, val_lr=0.002]   4%|▍         | 41/1000 [4:30:50<96:49:18, 363.46s/epoch, accuracy=0.684, loss=0.88, lr=0.002, val_accuracy=0.694, val_loss=0.871, val_lr=0.002]  4%|▍         | 42/1000 [4:36:58<97:07:18, 364.97s/epoch, accuracy=0.684, loss=0.88, lr=0.002, val_accuracy=0.692, val_loss=0.874, val_lr=0.002]  4%|▍         | 43/1000 [4:43:10<97:34:14, 367.04s/epoch, accuracy=0.684, loss=0.88, lr=0.002, val_accuracy=0.689, val_loss=0.87, val_lr=0.002]   4%|▍         | 44/1000 [4:49:24<98:03:04, 369.23s/epoch, accuracy=0.684, loss=0.88, lr=0.002, val_accuracy=0.692, val_loss=0.875, val_lr=0.002]  4%|▍         | 45/1000 [4:55:35<98:04:20, 369.70s/epoch, accuracy=0.684, loss=0.88, lr=0.002, val_accuracy=0.691, val_loss=0.867, val_lr=0.002]  5%|▍         | 46/1000 [5:01:51<98:29:20, 371.66s/epoch, accuracy=0.684, loss=0.879, lr=0.002, val_accuracy=0.694, val_loss=0.864, val_lr=0.002]  5%|▍         | 47/1000 [5:08:09<98:52:54, 373.53s/epoch, accuracy=0.684, loss=0.879, lr=0.002, val_accuracy=0.694, val_loss=0.867, val_lr=0.002]  5%|▍         | 48/1000 [5:14:29<99:17:45, 375.49s/epoch, accuracy=0.685, loss=0.879, lr=0.002, val_accuracy=0.686, val_loss=0.873, val_lr=0.002]  5%|▍         | 49/1000 [5:21:15<101:36:56, 384.67s/epoch, accuracy=0.684, loss=0.879, lr=0.002, val_accuracy=0.691, val_loss=0.868, val_lr=0.002]  5%|▌         | 50/1000 [5:28:06<103:34:07, 392.47s/epoch, accuracy=0.684, loss=0.878, lr=0.002, val_accuracy=0.694, val_loss=0.864, val_lr=0.002]  5%|▌         | 51/1000 [5:34:50<104:21:17, 395.87s/epoch, accuracy=0.685, loss=0.878, lr=0.002, val_accuracy=0.694, val_loss=0.868, val_lr=0.002]  5%|▌         | 52/1000 [5:41:33<104:50:58, 398.16s/epoch, accuracy=0.685, loss=0.878, lr=0.002, val_accuracy=0.692, val_loss=0.865, val_lr=0.002]  5%|▌         | 53/1000 [5:48:15<105:01:10, 399.23s/epoch, accuracy=0.685, loss=0.878, lr=0.002, val_accuracy=0.679, val_loss=0.889, val_lr=0.002]  5%|▌         | 54/1000 [5:55:10<106:10:36, 404.06s/epoch, accuracy=0.685, loss=0.878, lr=0.002, val_accuracy=0.694, val_loss=0.867, val_lr=0.002]  6%|▌         | 55/1000 [6:02:09<107:11:46, 408.37s/epoch, accuracy=0.685, loss=0.877, lr=0.002, val_accuracy=0.691, val_loss=0.866, val_lr=0.002]  6%|▌         | 56/1000 [6:09:01<107:24:34, 409.61s/epoch, accuracy=0.685, loss=0.878, lr=0.002, val_accuracy=0.686, val_loss=0.875, val_lr=0.002]  6%|▌         | 57/1000 [6:15:57<107:48:29, 411.57s/epoch, accuracy=0.685, loss=0.877, lr=0.002, val_accuracy=0.695, val_loss=0.865, val_lr=0.002]  6%|▌         | 58/1000 [6:22:56<108:14:48, 413.68s/epoch, accuracy=0.685, loss=0.877, lr=0.002, val_accuracy=0.683, val_loss=0.901, val_lr=0.002]  6%|▌         | 59/1000 [6:29:52<108:18:00, 414.33s/epoch, accuracy=0.685, loss=0.877, lr=0.002, val_accuracy=0.685, val_loss=0.877, val_lr=0.002]  6%|▌         | 60/1000 [6:36:50<108:28:50, 415.46s/epoch, accuracy=0.685, loss=0.876, lr=0.002, val_accuracy=0.69, val_loss=0.869, val_lr=0.002]   6%|▌         | 61/1000 [6:43:48<108:36:14, 416.37s/epoch, accuracy=0.685, loss=0.877, lr=0.002, val_accuracy=0.691, val_loss=0.866, val_lr=0.002]  6%|▌         | 62/1000 [6:50:47<108:37:25, 416.89s/epoch, accuracy=0.685, loss=0.876, lr=0.002, val_accuracy=0.688, val_loss=0.87, val_lr=0.002]   6%|▋         | 63/1000 [6:57:42<108:22:50, 416.40s/epoch, accuracy=0.686, loss=0.876, lr=0.002, val_accuracy=0.693, val_loss=0.864, val_lr=0.002]  6%|▋         | 64/1000 [7:04:34<107:54:23, 415.02s/epoch, accuracy=0.685, loss=0.876, lr=0.002, val_accuracy=0.69, val_loss=0.868, val_lr=0.002]   6%|▋         | 65/1000 [7:11:26<107:33:43, 414.14s/epoch, accuracy=0.686, loss=0.876, lr=0.002, val_accuracy=0.69, val_loss=0.87, val_lr=0.002]   7%|▋         | 66/1000 [7:18:17<107:12:55, 413.25s/epoch, accuracy=0.686, loss=0.876, lr=0.002, val_accuracy=0.693, val_loss=0.866, val_lr=0.002]  7%|▋         | 67/1000 [7:24:54<105:51:59, 408.49s/epoch, accuracy=0.685, loss=0.876, lr=0.002, val_accuracy=0.694, val_loss=0.864, val_lr=0.002]  7%|▋         | 68/1000 [7:31:29<104:41:23, 404.38s/epoch, accuracy=0.687, loss=0.872, lr=0.0002, val_accuracy=0.694, val_loss=0.861, val_lr=0.0002]  7%|▋         | 69/1000 [7:38:01<103:38:09, 400.74s/epoch, accuracy=0.687, loss=0.872, lr=0.0002, val_accuracy=0.694, val_loss=0.862, val_lr=0.0002]  7%|▋         | 70/1000 [7:44:28<102:24:40, 396.43s/epoch, accuracy=0.687, loss=0.872, lr=0.0002, val_accuracy=0.696, val_loss=0.861, val_lr=0.0002]  7%|▋         | 71/1000 [7:50:54<101:30:44, 393.37s/epoch, accuracy=0.687, loss=0.872, lr=0.0002, val_accuracy=0.694, val_loss=0.862, val_lr=0.0002]  7%|▋         | 72/1000 [7:57:21<100:53:34, 391.40s/epoch, accuracy=0.687, loss=0.872, lr=0.0002, val_accuracy=0.695, val_loss=0.861, val_lr=0.0002]  7%|▋         | 73/1000 [8:03:47<100:24:51, 389.96s/epoch, accuracy=0.687, loss=0.872, lr=0.0002, val_accuracy=0.695, val_loss=0.86, val_lr=0.0002]   7%|▋         | 74/1000 [8:10:13<99:59:34, 388.74s/epoch, accuracy=0.687, loss=0.872, lr=0.0002, val_accuracy=0.694, val_loss=0.861, val_lr=0.0002]  8%|▊         | 75/1000 [8:17:05<101:37:23, 395.51s/epoch, accuracy=0.687, loss=0.872, lr=0.0002, val_accuracy=0.695, val_loss=0.862, val_lr=0.0002]  8%|▊         | 76/1000 [8:23:54<102:33:13, 399.56s/epoch, accuracy=0.687, loss=0.872, lr=0.0002, val_accuracy=0.695, val_loss=0.861, val_lr=0.0002]  8%|▊         | 77/1000 [8:30:30<102:13:16, 398.70s/epoch, accuracy=0.687, loss=0.872, lr=0.0002, val_accuracy=0.694, val_loss=0.862, val_lr=0.0002]  8%|▊         | 78/1000 [8:37:07<101:57:23, 398.09s/epoch, accuracy=0.688, loss=0.872, lr=0.0002, val_accuracy=0.696, val_loss=0.86, val_lr=0.0002]   8%|▊         | 79/1000 [8:43:36<101:08:49, 395.36s/epoch, accuracy=0.687, loss=0.872, lr=0.0002, val_accuracy=0.693, val_loss=0.861, val_lr=0.0002]  8%|▊         | 80/1000 [8:50:02<100:18:36, 392.52s/epoch, accuracy=0.687, loss=0.872, lr=0.0002, val_accuracy=0.695, val_loss=0.861, val_lr=0.0002]  8%|▊         | 81/1000 [8:56:26<99:34:46, 390.08s/epoch, accuracy=0.687, loss=0.871, lr=2e-5, val_accuracy=0.695, val_loss=0.861, val_lr=2e-5]       8%|▊         | 82/1000 [9:02:44<98:33:03, 386.47s/epoch, accuracy=0.687, loss=0.871, lr=2e-5, val_accuracy=0.695, val_loss=0.86, val_lr=2e-5]   8%|▊         | 83/1000 [9:08:59<97:32:36, 382.94s/epoch, accuracy=0.688, loss=0.871, lr=2e-5, val_accuracy=0.695, val_loss=0.86, val_lr=2e-5]  8%|▊         | 84/1000 [9:15:14<96:51:51, 380.69s/epoch, accuracy=0.687, loss=0.871, lr=2e-5, val_accuracy=0.695, val_loss=0.86, val_lr=2e-5]  8%|▊         | 85/1000 [9:22:02<98:47:41, 388.70s/epoch, accuracy=0.688, loss=0.871, lr=2e-5, val_accuracy=0.695, val_loss=0.861, val_lr=2e-5]  9%|▊         | 86/1000 [9:28:16<97:32:49, 384.21s/epoch, accuracy=0.688, loss=0.871, lr=2e-5, val_accuracy=0.695, val_loss=0.86, val_lr=2e-5]   9%|▊         | 87/1000 [9:34:43<97:39:44, 385.09s/epoch, accuracy=0.688, loss=0.871, lr=2e-5, val_accuracy=0.695, val_loss=0.86, val_lr=2e-5]  9%|▉         | 88/1000 [9:41:03<97:10:27, 383.58s/epoch, accuracy=0.688, loss=0.871, lr=2e-5, val_accuracy=0.695, val_loss=0.861, val_lr=2e-5]  9%|▉         | 89/1000 [9:47:18<96:24:10, 380.96s/epoch, accuracy=0.688, loss=0.871, lr=2e-5, val_accuracy=0.695, val_loss=0.861, val_lr=2e-5]  9%|▉         | 90/1000 [9:53:26<95:23:01, 377.34s/epoch, accuracy=0.687, loss=0.871, lr=2e-5, val_accuracy=0.696, val_loss=0.86, val_lr=2e-5]   9%|▉         | 90/1000 [9:53:26<100:00:25, 395.63s/epoch, accuracy=0.687, loss=0.871, lr=2e-5, val_accuracy=0.696, val_loss=0.86, val_lr=2e-5]


Running train for /cluster/home/okulseng/PELICAN/experiments/_present/log-nano-pelican-mlp/conf-pelican-3/config.yml


Model: "functional_3"
┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)        ┃ Output Shape      ┃    Param # ┃ Connected to      ┃
┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩
│ input_layer_1       │ (None, 32, 4)     │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ inner_product_1     │ [(None, 32, 32,   │          0 │ input_layer_1[0]… │
│ (InnerProduct)      │ 1), (None, 32,    │            │                   │
│                     │ 32, 1)]           │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ log_layer_1         │ (None, 32, 32, 1) │          1 │ inner_product_1[… │
│ (LogLayer)          │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 32, 32, 1) │          4 │ log_layer_1[0][0] │
│ (BatchNormalizatio… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ lineq2v2_1          │ (None, 32, 32, 6) │          0 │ batch_normalizat… │
│ (Lineq2v2)          │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ diag_bias_dense_1   │ (None, 32, 32, 8) │         64 │ lineq2v2_1[0][0]  │
│ (DiagBiasDense)     │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ multiply_1          │ (None, 32, 32, 8) │          0 │ diag_bias_dense_… │
│ (Multiply)          │                   │            │ inner_product_1[… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ batch_normalizatio… │ (None, 32, 32, 8) │         32 │ multiply_1[0][0]  │
│ (BatchNormalizatio… │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ lineq2v0_1          │ (None, 16)        │          0 │ batch_normalizat… │
│ (Lineq2v0)          │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_1 (Dense)     │ (None, 5)         │         85 │ lineq2v0_1[0][0]  │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 186 (744.00 B)
 Trainable params: 168 (672.00 B)
 Non-trainable params: 18 (72.00 B)
'LogLayer' object has no attribute 'calc_flops'
0epoch [00:00, ?epoch/s]  0%|          | 0/1000 [00:00<?, ?epoch/s]  0%|          | 1/1000 [06:25<106:55:01, 385.29s/epoch, accuracy=0.578, loss=1.13, lr=0.002, val_accuracy=0.664, val_loss=0.982, val_lr=0.002]slurmstepd: error: *** JOB 61661106 ON eu-g5-031-2 CANCELLED AT 2024-06-09T03:34:20 DUE TO TIME LIMIT ***
