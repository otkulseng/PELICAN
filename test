#!/usr/bin/env python

import argparse
import yaml
import logging
from pathlib import Path
import os
import pickle
from tqdm import tqdm

import matplotlib.pyplot as plt
import pandas as pd

from nanopelican.models import PelicanNano
from nanopelican.data import load_dataset
from nanopelican.models import load_model, load_history

from nanopelican.schedulers import LinearWarmupCosineAnnealing

from tqdm.keras import TqdmCallback

from keras import callbacks

from keras.optimizers import AdamW, Adam
from keras.losses import CategoricalCrossentropy, BinaryCrossentropy
from keras.callbacks import CSVLogger

logger = logging.getLogger('')

def load_arguments():
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("--dir", required=True,  type=str)
    parser.add_argument("--name",  type=str, default='')

    args = parser.parse_args()
    return args

def load_yaml(filename):
    with open(filename, "r") as stream:
        config = yaml.load(stream, Loader=yaml.Loader)
    return config


from sklearn.metrics import roc_curve, auc
from scipy.special import softmax

def generate_auc(model, data):
    y_score = model.predict(data.x_data)
    y_true = data.y_data

    fig, ax = plt.subplots()
    fpr, tpr, thresholds = roc_curve(y_true, y_score)
    score = auc(fpr, tpr)
    ax.plot(fpr, tpr)
    fig.suptitle(f'AUC {round(score, 4)}')
    ax.set_aspect('equal')
    return fig, (fpr, tpr, score)

def generate_plot(filename):
    file = pd.read_csv(filename)
    fig, (axL, axR) = plt.subplots(ncols=2, figsize=(10, 5))
    for key in file:
        if 'acc' in key:
            axL.plot(file[key], label=key)

        if 'loss' in key:
            axR.plot(file[key], label=key)

    axL.set_title('Accuracy')
    axL.legend(loc='lower right')

    axR.set_title('Loss')
    axR.legend()
    fig.supxlabel('Epochs')
    return fig

def run_test(experiment):
    print(experiment, experiment.is_dir())
    logger.setLevel(logging.INFO)
    logfile = logging.FileHandler(experiment / 'test.log')
    logging.basicConfig(level=logging.INFO, handlers=[logfile])

    logger.info('-' * 10 + 'START TEST' + 10 * '-')

    config = load_yaml(experiment / 'config.yml')
    dataset = load_dataset(config['dataset'], keys=['test'])


    for file in experiment.iterdir():
        name = file.name
        if '.keras' not in name:
            continue
        try:
            logger.info(f'Testing {name}')
            model = load_model(file)
            loss, acc = model.evaluate(dataset.test.batch(1000))

            fig, (fpr, tpr, auc) = generate_auc(model, dataset.test)
            logger.info(f'{name}: finish \n loss: {loss} accuracy: {acc} AUC: {auc}')
            # logger.info(f'{name}: FPR: {fpr} TPR: {tpr}')

            fig.savefig(experiment / f'{name} - auc.pdf')
        except Exception as e:
            logger.info(f'Failed with error {str(e)}')
            continue

    logger.info('-' * 10 + 'STOP TEST' + 10 * '-')

    fig = generate_plot(experiment / 'training.log')
    fig.savefig(experiment / 'plot.pdf')


def main():
    args = load_arguments()

    # root = Path.cwd()
    found = False
    for elem in Path.cwd().iterdir():
        if elem.is_dir() and args.dir in elem.name:
            root = elem
            found = True
            break
    if not found:
        raise FileNotFoundError(f"Cannot find directory {args.dir}")


    experiments = []
    for file in root.iterdir():
        if file.is_dir() and args.name in file.name:
            experiments.append(file)

    print(f'Found {len(experiments)} tests')

    for exp in tqdm(experiments):
        try:
            run_test(exp)
        except KeyboardInterrupt:
            break
        except :
            continue

if __name__ == '__main__':
    main()

